{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906b27ef",
   "metadata": {},
   "source": [
    "# 🔍 Discover‑then‑Name: *Task‑Agnostic Concept Bottleneck Models*\n",
    "\n",
    "**Hands‑On Tutorial Notebook – KDD 2025 ‘Beyond Feature Attribution’**  \n",
    "*Based on*: Rao *et al.* (2024) *Discover‑then‑Name* (ECCV).  \n",
    "*Official repo*: <https://github.com/neuroexplicit-saar/discover-then-name>\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cxai-mechint-htutorial-kdd2025/cxai-mechint-htutorial-kdd2025.github.io/blob/main/notebooks/03_discover_then_name.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "---\n",
    "### 🌟 What you will learn\n",
    "1. **Discover** monosemantic latent concepts in a vision model using a Sparse Autoencoder (SAE).\n",
    "2. **Name** those concepts automatically via CLIP text embeddings.\n",
    "\n",
    "**Estimated runtime** (Colab, T4 GPU): ≈&nbsp;15 min using pretrained checkpoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d65a7",
   "metadata": {},
   "source": [
    "## 🗺️ Notebook Roadmap\n",
    "1. **Setup & Dependencies**  \n",
    "2. **Load CLIP ViT‑B/32**  \n",
    "3. **Prepare CIFAR‑100 probe dataset**  \n",
    "4. **Discover Concepts** (load pretrained SAE)  \n",
    "5. **Visualize & Name Concepts**  \n",
    "6. **Exercises & References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b11cf5",
   "metadata": {},
   "source": [
    "## 1 · Environment Setup\n",
    "👉 **Run the cell below** on Colab (GPU runtime) to clone the repo and install all requirements. On a local machine, make sure you have CUDA‑enabled PyTorch ≥ 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository only if not already done\n",
    "import os\n",
    "if not os.path.exists('discover-then-name'):\n",
    "    !git clone https://github.com/neuroexplicit-saar/discover-then-name.git\n",
    "%cd discover-then-name\n",
    "\n",
    "import torch, random, numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e31026",
   "metadata": {},
   "source": [
    "## 2 · Load CLIP Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18060a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip, torch\n",
    "clip_model, preprocess = clip.load('ViT-B/16', device=device)\n",
    "clip_model.eval(); print('✅ CLIP ViT‑B/16 loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48376d39",
   "metadata": {},
   "source": [
    "## 3 · Prepare CIFAR‑100 Probe Dataset\n",
    "We will embed **~10 000** validation images from CIFAR‑100 with CLIP. This step takes < 1 min on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1882062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the probe dataset, CIFAR100\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar100_dataset = datasets.CIFAR100(root='data', train=False, download=True, transform=preprocess)\n",
    "cifar100_loader = DataLoader(cifar100_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'Loaded CIFAR100 dataset with {len(cifar100_dataset)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ecc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the probe encodings using CLIP\n",
    "import numpy as np\n",
    "probe_encodings = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in cifar100_loader:\n",
    "        images = images.to(device)\n",
    "        encodings = clip_model.encode_image(images).cpu().numpy()\n",
    "        probe_encodings.append(encodings)\n",
    "probe_encodings = np.concatenate(probe_encodings, axis=0)\n",
    "print(f'Collected {len(probe_encodings)} probe encodings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b20e21",
   "metadata": {},
   "source": [
    "## 4 · Discover Latent Concepts with Sparse Autoencoder\n",
    "To save time, we download a **pretrained SAE** trained on millions of CLIP embeddings. If the download fails (e.g., offline), a small fallback SAE will be trained quickly (low quality but runnable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Sparse Autoencoder\n",
    "from sparse_autoencoder import SparseAutoencoder\n",
    "\n",
    "sae = SparseAutoencoder(n_input_features=512, n_learned_features=4096, n_components=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2076f1",
   "metadata": {},
   "source": [
    "### 3.1 . Train the autoencoder with the probe dataset activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffe454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_autoencoder import (\n",
    "    L2ReconstructionLoss,\n",
    "    LearnedActivationsL1Loss,\n",
    "    LossReducer,\n",
    "    LossReductionType\n",
    ")\n",
    "loss = LossReducer(\n",
    "    LearnedActivationsL1Loss(l1_coefficient=0.0003),\n",
    "    L2ReconstructionLoss())\n",
    "\n",
    "optim = torch.optim.Adam(sae.parameters(), lr=1e-3)\n",
    "# extend the batch dimension to match the expected input shape\n",
    "data = torch.tensor(probe_encodings, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "for epoch in range(30):\n",
    "    learned_activations, reconstructed_activations = sae.forward(data)\n",
    "    total_loss, loss_metrics = loss.scalar_loss_with_log(data, learned_activations, reconstructed_activations,component_reduction=LossReductionType.MEAN)\n",
    "    optim.zero_grad(); total_loss.backward(); optim.step()\n",
    "    print(f'Epoch {epoch+1} – loss: {total_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426897fb",
   "metadata": {},
   "source": [
    "### 3.2 . Load a pre-trained encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04486437",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../Checkpoints/clip_ViT-B:16_sparse_autoencoder_final.pt\", map_location=device)\n",
    "sae.load_state_dict(state_dict)\n",
    "sae.eval()\n",
    "print('✅ Pretrained Sparse Autoencoder loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947da47b",
   "metadata": {},
   "source": [
    "### 3.3 . Load the concept names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concept names\n",
    "with open(\"../Assigned Names/clip_ViT-B:16_concept_names.csv\", \"r\") as f:\n",
    "    concept_names = f.read().splitlines()\n",
    "concept_names = [name.split(',')[1] for name in concept_names]\n",
    "concept_indexes = dict(zip(concept_names, range(len(concept_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['fences', 'pupil', 'doors', 'bed']\n",
    "\n",
    "# concept_idx = 1526 # fences\n",
    "# concept_idx = 3955 # pupil\n",
    "# concept_idx = 704 # doors\n",
    "concept_idx = concept_indexes['bed'] #2061 # bed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b8d2f",
   "metadata": {},
   "source": [
    "### 4.1 · Visualize Learned Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "weights = sae.decoder.weight.detach().cpu()\n",
    "# neurons = weights.norm(p=2, dim=1).topk(6).indices\n",
    "neurons = [concept_indexes[name] for name in vocab if name in concept_indexes]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for ax, idx in zip(axes.flatten(), neurons):\n",
    "    ax.imshow(weights[0,:,idx].view(32,16), cmap='viridis', aspect='auto')\n",
    "    ax.set_title(f'Neuron {idx}: {concept_names[idx]}')\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Random projections of decoder rows'); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda99d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.detach().cpu().numpy()[0].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 2D projection of the decoder weights using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0, max_iter=1000, perplexity=30)\n",
    "weights_2d = tsne.fit_transform(weights.detach().cpu().numpy()[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [concept_indexes[name] for i, name in enumerate(concept_indexes.keys()) if i%2==0]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.scatter(weights_2d[:, 0], weights_2d[:, 1], s=5, alpha=0.5)\n",
    "for idx in neurons:\n",
    "    plt.annotate(concept_names[idx], (weights_2d[idx, 0], weights_2d[idx, 1]), fontsize=8)\n",
    "plt.title('t-SNE projection of decoder weights')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e44d1",
   "metadata": {},
   "source": [
    "## 5 . Extract concepts from the probe dataset using the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(probe_encodings, device=device, dtype=torch.float32)\n",
    "sae.eval()\n",
    "with torch.no_grad():\n",
    "    concepts, reconstructions = sae(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4a3f0",
   "metadata": {},
   "source": [
    "## 5 . Show top images for selected concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d41e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_dataset_orig = datasets.CIFAR100(root='data', train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_idx = concept_indexes['fences']\n",
    "# concept_idx = concept_indexes['pupil']\n",
    "# concept_idx = concept_indexes['doors']\n",
    "# concept_idx = concept_indexes['bed']\n",
    "\n",
    "concept_names[concept_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_strengths = concepts[:, 0, concept_idx].cpu().numpy()\n",
    "top_indices = concept_strengths.argsort()[::-1][:10]\n",
    "top_images = [cifar100_dataset_orig[i][0] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895dc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top images for concept 1526\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, 5, figsize=(8, 3))\n",
    "for i, img_idx in enumerate(top_indices):\n",
    "    img = cifar100_dataset_orig[img_idx][0]\n",
    "    # img = img.permute(1, 2, 0).numpy()  # Convert to HWC format for plotting\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Concept Strength: {concept_strengths[top_indices[i]]:.4f}\\nImage: {img_idx}\", fontsize=8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90099f71",
   "metadata": {},
   "source": [
    "## 8 · Exercises & Further Reading\n",
    "1. **Improve concept naming** by providing a richer vocabulary—e.g., the 50 000 CLIP tokens—and measuring naming recall.\n",
    "2. **Tune sparsity** (`l1_lambda`) or **hidden size** and observe effects on interpretability vs. accuracy.\n",
    "3. Replace CIFAR‑100 with your own dataset (or **ImageNet‑mini**) and retrain the DN‑CBM.\n",
    "4. Combine **TCAV** with the discovered concepts to quantify their directional influence on model predictions.\n",
    "\n",
    "---\n",
    "### 📑 References\n",
    "- Rao *et al.* (2024) *Discover‑then‑Name: Task‑Agnostic Concept Bottlenecks via Automated Concept Discovery.* ECCV.\n",
    "- Kim *et al.* (2018) *TCAV: Quantitative Testing with Concept Activation Vectors.* ICML.\n",
    "- Oikarinen *et al.* (2023) *Label‑Free Concept Bottleneck Models.* arXiv.\n",
    "- Radford *et al.* (2021) *Learning Transferable Visual Models from Natural Language Supervision.* ICML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
